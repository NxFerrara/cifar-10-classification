{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:03.307990Z",
     "iopub.status.busy": "2025-03-22T01:25:03.307551Z",
     "iopub.status.idle": "2025-03-22T01:25:03.704193Z",
     "shell.execute_reply": "2025-03-22T01:25:03.703250Z",
     "shell.execute_reply.started": "2025-03-22T01:25:03.307940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:03.705742Z",
     "iopub.status.busy": "2025-03-22T01:25:03.705273Z",
     "iopub.status.idle": "2025-03-22T01:25:04.543183Z",
     "shell.execute_reply": "2025-03-22T01:25:04.542179Z",
     "shell.execute_reply.started": "2025-03-22T01:25:03.705716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Specify the folder where the CIFAR-10 batch files are\n",
    "cifar10_dir = '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py'\n",
    "\n",
    "# Load the label names\n",
    "meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\n",
    "label_names = meta_data_dict[b'label_names']\n",
    "\n",
    "# Load one batch for demonstration (e.g., data_batch_1)\n",
    "batch_1_dict = load_cifar_batch(os.path.join(cifar10_dir, 'data_batch_1'))\n",
    "train_images = batch_1_dict[b'data']\n",
    "train_labels = batch_1_dict[b'labels']\n",
    "\n",
    "# Reshape the images\n",
    "train_images = train_images.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "\n",
    "# Display the first 10 images and labels\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.title(label_names[train_labels[i]].decode('utf-8'))  # Decoding from bytes to string\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:04.545114Z",
     "iopub.status.busy": "2025-03-22T01:25:04.544905Z",
     "iopub.status.idle": "2025-03-22T01:25:04.719689Z",
     "shell.execute_reply": "2025-03-22T01:25:04.718795Z",
     "shell.execute_reply.started": "2025-03-22T01:25:04.545096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the test file, note that it has no labels and needs to be used with your model inference to predict outputs.\n",
    "\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "# Load the batch\n",
    "cifar10_batch = load_cifar_batch('/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl')\n",
    "\n",
    "# Extract images \n",
    "images = cifar10_batch[b'data']\n",
    "# Unlike the train images you are not required to reshape to (number of images, width, height, channels) \n",
    "# as the test data is already in (N x W x H x C) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:04.721065Z",
     "iopub.status.busy": "2025-03-22T01:25:04.720757Z",
     "iopub.status.idle": "2025-03-22T01:25:05.042538Z",
     "shell.execute_reply": "2025-03-22T01:25:05.041608Z",
     "shell.execute_reply.started": "2025-03-22T01:25:04.721036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display the first 10 images\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:05.044061Z",
     "iopub.status.busy": "2025-03-22T01:25:05.043804Z",
     "iopub.status.idle": "2025-03-22T01:25:12.375441Z",
     "shell.execute_reply": "2025-03-22T01:25:12.374545Z",
     "shell.execute_reply.started": "2025-03-22T01:25:05.044038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(128)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(128)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.376797Z",
     "iopub.status.busy": "2025-03-22T01:25:12.376360Z",
     "iopub.status.idle": "2025-03-22T01:25:12.383298Z",
     "shell.execute_reply": "2025-03-22T01:25:12.382390Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.376772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Implementation of a residual block.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, activation=nn.ReLU):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # Main convolution path\n",
    "        self.main_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                     stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        # Skip connection path\n",
    "        self.skip_connection = self._create_skip_connection(\n",
    "            in_channels, out_channels, stride)\n",
    "        \n",
    "        # Final activation\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def _create_skip_connection(self, in_channels, out_channels, stride):\n",
    "        # No downsampling needed if dimensions match\n",
    "        if stride == 1 and in_channels == out_channels:\n",
    "            return nn.Identity()\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                     stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = self.main_path(x)\n",
    "        \n",
    "        # Skip connection\n",
    "        identity = self.skip_connection(x)\n",
    "        \n",
    "        # Combine and activate\n",
    "        return self.activation(out + identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.384299Z",
     "iopub.status.busy": "2025-03-22T01:25:12.384014Z",
     "iopub.status.idle": "2025-03-22T01:25:12.406009Z",
     "shell.execute_reply": "2025-03-22T01:25:12.405382Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.384278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10, dropout_rate=0.1):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.in_channels = 48\n",
    "        self.conv1 = nn.Conv2d(3, 48, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 4 layers\n",
    "        self.channels = [48, 96, 128, 364]\n",
    "        self.layer1 = self._make_layer(block, self.channels[0], layers[0])\n",
    "        self.layer2 = self._make_layer(block, self.channels[1], layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, self.channels[2], layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, self.channels[3], layers[3], stride=2)\n",
    "        \n",
    "        # Fixed 4×4 average pooling\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
    "        \n",
    "        # Add dropout before final layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(self.channels[3], num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def custom_resnet18():\n",
    "    return CustomResNet(ResidualBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.408898Z",
     "iopub.status.busy": "2025-03-22T01:25:12.408701Z",
     "iopub.status.idle": "2025-03-22T01:25:12.420635Z",
     "shell.execute_reply": "2025-03-22T01:25:12.419764Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.408881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Display model summary and count the number of trainable parameters in the model\"\"\"\n",
    "    from torchsummary import summary\n",
    "    \n",
    "    # Display the detailed model summary for CIFAR-10 input size (3, 32, 32)\n",
    "    summary(model, (3, 32, 32))\n",
    "    \n",
    "    # Also return the total parameter count for convenience\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.422224Z",
     "iopub.status.busy": "2025-03-22T01:25:12.421940Z",
     "iopub.status.idle": "2025-03-22T01:25:12.436675Z",
     "shell.execute_reply": "2025-03-22T01:25:12.435862Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.422195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, OneCycleLR):\n",
    "            scheduler.step()\n",
    "        \n",
    "        loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    return loss / len(dataloader), 100 * correct / total, epoch_time\n",
    "\n",
    "def validate(model, dataloader, criterion, device, classes=None):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For confusion matrix\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Collect for confusion matrix\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate confusion matrix and classification report if classes are provided\n",
    "    results = {\n",
    "        'loss': loss / len(dataloader),\n",
    "        'accuracy': 100 * correct / total,\n",
    "        'time': epoch_time,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.437945Z",
     "iopub.status.busy": "2025-03-22T01:25:12.437645Z",
     "iopub.status.idle": "2025-03-22T01:25:12.459308Z",
     "shell.execute_reply": "2025-03-22T01:25:12.458722Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.437913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_predictions(model, test_images, device, transform_test):\n",
    "    \"\"\"Generate predictions for test images\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_images)):\n",
    "            # Standard prediction\n",
    "            img_tensor = transform_test(test_images[i]).unsqueeze(0).to(device)\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.append(predicted.item())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, lr_history=None):\n",
    "    \"\"\"\n",
    "    Plot training history including losses, accuracies and learning rate\n",
    "    \"\"\"\n",
    "    # Convert GPU tensors to CPU NumPy arrays if needed\n",
    "    def convert_to_numpy(data):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return data.cpu().detach().numpy()\n",
    "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], torch.Tensor):\n",
    "            return [x.cpu().detach().numpy() for x in data]\n",
    "        return data\n",
    "    \n",
    "    train_losses = convert_to_numpy(train_losses)\n",
    "    val_losses = convert_to_numpy(val_losses)\n",
    "    train_accs = convert_to_numpy(train_accs)\n",
    "    val_accs = convert_to_numpy(val_accs)\n",
    "    lr_history = convert_to_numpy(lr_history)\n",
    "    \n",
    "    # Determine the number of plots\n",
    "    num_plots = 3 if lr_history else 2\n",
    "    plt.figure(figsize=(12, 4*num_plots))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(num_plots, 1, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot learning rate if available\n",
    "    if lr_history:\n",
    "        plt.subplot(num_plots, 1, 3)\n",
    "        plt.plot(lr_history)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.460313Z",
     "iopub.status.busy": "2025-03-22T01:25:12.460054Z",
     "iopub.status.idle": "2025-03-22T01:25:12.477045Z",
     "shell.execute_reply": "2025-03-22T01:25:12.476380Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.460253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    \"\"\"Get the current learning rate from the optimizer\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def create_checkpoint(model, optimizer, scheduler, epoch, val_acc, val_loss, filename):\n",
    "    \"\"\"Create a checkpoint with all training state\"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None, device=None):\n",
    "    \"\"\"Load a checkpoint and restore training state\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if scheduler is not None and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    val_acc = checkpoint['val_acc']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {epoch} with validation accuracy: {val_acc:.2f}%\")\n",
    "    return epoch, val_acc, val_loss\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory usage if available\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        memory_cached = torch.cuda.memory_reserved() / 1024**2  # MB\n",
    "        return f\"GPU Memory: Allocated {memory_allocated:.2f} MB, Cached {memory_cached:.2f} MB\"\n",
    "    else:\n",
    "        return \"GPU not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.478089Z",
     "iopub.status.busy": "2025-03-22T01:25:12.477778Z",
     "iopub.status.idle": "2025-03-22T01:25:12.494309Z",
     "shell.execute_reply": "2025-03-22T01:25:12.493544Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.478054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up checkpoints directory\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:12.495369Z",
     "iopub.status.busy": "2025-03-22T01:25:12.495087Z",
     "iopub.status.idle": "2025-03-22T01:25:14.344791Z",
     "shell.execute_reply": "2025-03-22T01:25:14.343906Z",
     "shell.execute_reply.started": "2025-03-22T01:25:12.495342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset class for your pickle-loaded CIFAR-10\n",
    "class CIFAR10(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert to PIL image for transforms\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Load all training batches\n",
    "print(\"Loading CIFAR-10 dataset from pickle files...\")\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(1, 6):  # CIFAR-10 has 5 training batches\n",
    "    batch_file = os.path.join(cifar10_dir, f'data_batch_{i}')\n",
    "    batch_dict = load_cifar_batch(batch_file)\n",
    "    batch_images = batch_dict[b'data']\n",
    "    batch_images = batch_images.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    train_images.append(batch_images)\n",
    "    train_labels.extend(batch_dict[b'labels'])\n",
    "\n",
    "# Combine all batches into single numpy arrays\n",
    "train_images = np.concatenate(train_images, axis=0)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "print(f\"Training data shape: {train_images.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "\n",
    "def calculate_mean_std(images):\n",
    "    \"\"\"Calculate mean and std for a batch of images with shape [N, H, W, C]\"\"\"\n",
    "    # Convert to float and scale to [0, 1]\n",
    "    imgs = images.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Calculate mean and std across all images for each channel\n",
    "    means = np.mean(imgs, axis=(0, 1, 2))\n",
    "    stds = np.std(imgs, axis=(0, 1, 2))\n",
    "    \n",
    "    return means, stds\n",
    "\n",
    "# Calculate on your training images\n",
    "means, stds = calculate_mean_std(train_images)\n",
    "print(f\"Dataset mean: {means}, std: {stds}\")\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "# Create the custom dataset\n",
    "trainset = CIFAR10(train_images, train_labels, transform=transform_train)\n",
    "\n",
    "# Get class names for later visualization\n",
    "classes = trainset.classes\n",
    "\n",
    "# Create a validation set by splitting the training data\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# More efficient data loading with prefetching and multiple workers\n",
    "num_workers = 4  # Adjust based on your system\n",
    "prefetch_factor = 2  # Prefetch 2 batches per worker\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "# Load the test data from the pkl file\n",
    "test_dict = load_cifar_batch('/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl')\n",
    "test_images = test_dict[b'data']\n",
    "\n",
    "# Create dummy labels for the test set\n",
    "test_labels = np.zeros(len(test_images), dtype=np.int64)  \n",
    "\n",
    "testset = CIFAR10(test_images, test_labels, transform=transform_test)\n",
    "test_loader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:14.345901Z",
     "iopub.status.busy": "2025-03-22T01:25:14.345616Z",
     "iopub.status.idle": "2025-03-22T01:25:15.588132Z",
     "shell.execute_reply": "2025-03-22T01:25:15.587125Z",
     "shell.execute_reply.started": "2025-03-22T01:25:14.345867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = custom_resnet18().to(device)\n",
    "\n",
    "# Check parameter count\n",
    "param_count = count_parameters(model)\n",
    "assert param_count < 5_000_000, \"Model exceeds parameter limit of 5 million!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:15.589250Z",
     "iopub.status.busy": "2025-03-22T01:25:15.588990Z",
     "iopub.status.idle": "2025-03-22T01:25:15.597899Z",
     "shell.execute_reply": "2025-03-22T01:25:15.596970Z",
     "shell.execute_reply.started": "2025-03-22T01:25:15.589231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=0.1,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=100,\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "num_epochs = 100\n",
    "\n",
    "# For early stopping\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "lr_history = []\n",
    "\n",
    "# Resume from checkpoint if exists\n",
    "start_epoch = 0\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint at {checkpoint_path}\")\n",
    "    try:\n",
    "        start_epoch, best_val_acc, best_val_loss = load_checkpoint(\n",
    "            checkpoint_path, model, optimizer, scheduler, device\n",
    "        )\n",
    "        start_epoch += 1  # Start from the next epoch\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "print(f\"Initial model status: {check_gpu_memory()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T01:25:15.599066Z",
     "iopub.status.busy": "2025-03-22T01:25:15.598823Z",
     "iopub.status.idle": "2025-03-22T02:29:04.857773Z",
     "shell.execute_reply": "2025-03-22T02:29:04.856721Z",
     "shell.execute_reply.started": "2025-03-22T01:25:15.599046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_time = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scheduler\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_results = validate(model, val_loader, criterion, device, classes)\n",
    "    val_loss = val_results['loss']\n",
    "    val_acc = val_results['accuracy']\n",
    "    val_time = val_results['time']\n",
    "    \n",
    "    # Update learning rate for non-OneCycleLR schedulers\n",
    "    if scheduler is not None and not isinstance(scheduler, OneCycleLR):\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    lr_history.append(get_lr(optimizer))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
    "          f\"Time: {epoch_time:.2f}s, \"\n",
    "          f\"LR: {get_lr(optimizer):.6f}\")\n",
    "    \n",
    "    print(check_gpu_memory())\n",
    "    \n",
    "    # Check for best model\n",
    "    is_best = val_acc > best_val_acc\n",
    "    if is_best:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        create_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, val_acc, val_loss,\n",
    "            os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "        )\n",
    "        print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    # Save regular checkpoint\n",
    "    create_checkpoint(\n",
    "        model, optimizer, scheduler, epoch, val_acc, val_loss,\n",
    "        os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # Every 50 epochs, show confusion matrix\n",
    "    if (epoch + 1) % 50 == 0 or epoch == num_epochs - 1:\n",
    "        plot_confusion_matrix(\n",
    "            val_results['targets'],\n",
    "            val_results['predictions'],\n",
    "            classes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T02:29:04.859075Z",
     "iopub.status.busy": "2025-03-22T02:29:04.858808Z",
     "iopub.status.idle": "2025-03-22T02:29:05.517656Z",
     "shell.execute_reply": "2025-03-22T02:29:05.516743Z",
     "shell.execute_reply.started": "2025-03-22T02:29:04.859051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(train_losses, val_losses, train_accs, val_accs, lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T02:29:05.518871Z",
     "iopub.status.busy": "2025-03-22T02:29:05.518597Z",
     "iopub.status.idle": "2025-03-22T02:29:32.202081Z",
     "shell.execute_reply": "2025-03-22T02:29:32.201144Z",
     "shell.execute_reply.started": "2025-03-22T02:29:05.518836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best model for inference\n",
    "print(\"Loading best model for inference...\")\n",
    "best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "_, _, _ = load_checkpoint(best_model_path, model, device=device)\n",
    "\n",
    "# Generate classification report on validation set\n",
    "val_results = validate(model, val_loader, criterion, device, classes)\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(\n",
    "    val_results['targets'],\n",
    "    val_results['predictions'],\n",
    "    target_names=classes\n",
    "))\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = generate_test_predictions(model, images, device, transform_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(predictions)),\n",
    "    'label': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6894866,
     "sourceId": 11065070,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
